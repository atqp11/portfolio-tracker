# AI Model Router Implementation

**Status:** âœ… **Implemented** (November 20, 2025)
**Architecture:** Confidence-based routing with Groq + Gemini
**Strategy Document:** [docs/AI_MODEL_STRATEGY.md](../../docs/AI_MODEL_STRATEGY.md)

---

## Overview

This implementation provides a production-ready AI routing system that:
- Uses **Groq GPT-OSS 20B** for bulk processing (SEC filings, news, sentiment)
- Uses **Gemini Flash/Pro** for interactive chat with confidence-based escalation
- Tracks costs and performance metrics via telemetry logging
- Implements aggressive caching to meet budget targets ($35-78/month)

---

## File Structure

```
lib/ai/
â”œâ”€â”€ groq.ts                    # Groq service for bulk processing
â”œâ”€â”€ gemini.ts                  # Gemini service for chat
â”œâ”€â”€ confidence-router.ts       # Confidence-based routing logic
â”œâ”€â”€ router.ts                  # Legacy tier-based router (deprecated)
â””â”€â”€ README.md                  # This file

lib/telemetry/
â””â”€â”€ ai-logger.ts               # Telemetry tracking for costs/metrics

app/api/ai/
â”œâ”€â”€ chat/route.ts              # Chat endpoint with confidence routing
â”œâ”€â”€ generate/route.ts          # Legacy Gemini endpoint (uses old router)
â””â”€â”€ telemetry/ai/route.ts      # Telemetry stats API
```

---

## Usage

### 1. Investor Chat (Interactive)

**Use the confidence-based router for RAG-powered chat:**

```typescript
import { routeQueryWithConfidence } from '@/lib/ai/confidence-router';

const response = await routeQueryWithConfidence({
  userMessage: 'What are TSLA risk factors?',
  ragContext: '[Chunk 1] TSLA 10-K Item 1A: Risk factors include...',
  portfolio: {
    symbols: ['TSLA', 'AAPL'],
    totalValue: 50000,
  },
});

console.log(response.text);          // AI answer
console.log(response.confidence);    // 0-1 confidence score
console.log(response.escalated);     // true if escalated to Gemini Pro
console.log(response.cost);          // Cost in USD
console.log(response.sources);       // Cited sources [Chunk 1], [Chunk 2], etc.
```

**API endpoint:**

```bash
POST /api/ai/chat
Content-Type: application/json

{
  "message": "What are TSLA risk factors?",
  "ragContext": "[Chunk 1] TSLA 10-K Item 1A: ...",
  "portfolio": {
    "symbols": ["TSLA", "AAPL"],
    "totalValue": 50000
  }
}

# Response:
{
  "text": "TSLA's main risks include...",
  "confidence": 0.92,
  "model": "gemini-2.0-flash-exp",
  "sources": ["[Chunk 1]", "[Chunk 2]"],
  "escalated": false,
  "cost": 0.000042,
  "cached": false
}
```

---

### 2. Bulk Processing (SEC Filings, News)

**Use Groq directly for deterministic batch tasks:**

```typescript
import { summarizeFiling, analyzeNewsSentiment } from '@/lib/ai/groq';

// Summarize SEC filing chunks
const summary = await summarizeFiling(
  chunks,                // string[] of filing chunks
  '10-K',                // Filing type
  'Tesla Inc.'           // Company name
);

// Analyze news sentiment
const sentiment = await analyzeNewsSentiment(
  articles,              // Array of { headline, summary, source }
  'TSLA'                 // Stock symbol
);

console.log(sentiment.sentiment);    // 'positive' | 'neutral' | 'negative'
console.log(sentiment.keyPoints);    // ['...', '...', '...']
console.log(sentiment.relevance);    // 0.0 - 1.0
```

---

### 3. Telemetry Tracking

**Get AI usage stats:**

```typescript
import { getTelemetryStats, checkMetricThresholds } from '@/lib/telemetry/ai-logger';

// Get stats for last 24 hours
const stats = getTelemetryStats();

console.log(stats.totalRequests);      // 1234
console.log(stats.cacheHitRate);       // 0.87 (87%)
console.log(stats.escalationRate);     // 0.08 (8%)
console.log(stats.totalCostUsd);       // $2.34
console.log(stats.avgLatencyMs);       // 1250ms
console.log(stats.p95LatencyMs);       // 3200ms

// Check if metrics are within targets
const warnings = checkMetricThresholds(stats);
warnings.forEach(w => console.warn(w));
// âš ï¸ Cache hit rate 78.5% < 80% target
```

**API endpoint:**

```bash
GET /api/telemetry/ai?period=24h

# Response:
{
  "period": "24h",
  "stats": {
    "totalRequests": 1234,
    "cacheHitRate": 0.87,
    "escalationRate": 0.08,
    "totalCostUsd": 2.34,
    ...
  },
  "warnings": ["âš ï¸ P95 latency 7200ms > 7s target"],
  "recentLogs": [...]
}
```

---

## Environment Variables

**Required:**

```bash
# Groq (bulk processing)
GROQ_API_KEY=gsk_...

# Google Gemini (chat)
GEMINI_API_KEY=AIza...
# OR use legacy key
AI_API_KEY=AIza...
```

**Get API keys:**
- Groq: https://console.groq.com
- Google Gemini: https://ai.google.dev

---

## Confidence-Based Routing Logic

**Decision flow (from AI_MODEL_STRATEGY.md):**

```
1. User asks question â†’ Retrieve RAG context
   â†“
2. Call primary model (Gemini Flash) with temp=0.3
   â†“
3. Extract confidence score + validate numeric claims
   â†“
4. IF confidence â‰¥0.85:
     â†’ Accept and cache for 12-24 hours âœ…
   â†“
5. ELSE IF 0.6 â‰¤ confidence <0.85:
     â†’ Escalate to Gemini Pro (temp=0.0) â¬†ï¸
     â†’ Re-run and accept if improved
   â†“
6. ELSE IF confidence <0.6:
     â†’ Escalate to Gemini Pro
     â†’ Flag for manual review âš ï¸
   â†“
7. Log all escalations for feedback loop
```

**Confidence scoring:**

```typescript
function estimateConfidence(response: string, context: string): number {
  let score = 0.5; // baseline

  // Boost confidence if:
  if (hasCitations)        score += 0.2;  // [Chunk 1], (per TSLA 10-K)
  if (hasSpecificNumbers)  score += 0.15; // $2.3B, 25%, etc.
  if (hasCoherentReasoning) score += 0.15; // >100 chars, not truncated
  if (hasMultipleSources)  score += 0.1;  // â‰¥3 chunk citations

  // Reduce confidence if:
  if (hasUncertainty)      score -= 0.2;  // "I think", "maybe", "possibly"
  if (missingContext)      score -= 0.15; // context <100 chars
  if (hasContradictions)   score -= 0.1;  // "however", "but" with short text

  return Math.max(0, Math.min(1, score));
}
```

---

## Cost Tracking

**Target metrics (from AI_MODEL_STRATEGY.md):**

| Metric | Target | Alert Threshold |
|--------|--------|----------------|
| **Cache hit rate** | >85% | <80% |
| **Escalation rate** | <10% | >15% |
| **Avg cost per request** | <$0.05 | >$0.10 |
| **P50 latency (chat)** | <1.5s | >3s |
| **P95 latency (chat)** | <4s | >7s |
| **Daily cost (total)** | <$3 | >$5 |
| **Monthly cost (total)** | <$60 | >$100 |

**Example telemetry log:**

```
ğŸ¤– AI Log: gemini/gemini-2.0-flash-exp | chat | 1250ms | 0.0042Â¢ | conf:0.92 | FRESH
ğŸ¤– AI Log: gemini/gemini-1.5-pro | chat | 2100ms | 0.0380Â¢ | conf:0.88 | FRESH [ESCALATED]
ğŸ¤– AI Log: groq/llama3-groq-70b-8192 | filing_summary | 450ms | 0.0018Â¢ | conf:0.95 | FRESH
ğŸ¤– AI Log: gemini/gemini-2.0-flash-exp | chat | 120ms | 0.0000Â¢ | conf:0.92 | CACHE
```

---

## Migration from Old Router

**The old tier-based router (`lib/ai/router.ts`) is deprecated.**

**Changes:**

| Old (Tier-Based) | New (Confidence-Based) |
|------------------|------------------------|
| `routeQuery(query, 'tier1')` | `routeQueryWithConfidence({ userMessage: query })` |
| 3 tiers (tier1, tier2, tier3) | Confidence thresholds (0.85, 0.60) |
| OpenRouter models (DeepSeek, Qwen, Llama) | Groq (bulk) + Gemini (chat) |
| Query-based tier selection | Confidence-based escalation |

**To migrate:**

1. Replace `import { routeQuery } from '@/lib/ai/router'` with:
   ```typescript
   import { routeQueryWithConfidence } from '@/lib/ai/confidence-router';
   ```

2. Update function calls:
   ```typescript
   // Old
   const { tier, model } = routeQuery(query, 'tier1');

   // New
   const response = await routeQueryWithConfidence({
     userMessage: query,
     ragContext: context,
   });
   ```

3. The old `/api/ai/generate` route still uses the old router. **TODO: Migrate to confidence router.**

---

## Next Steps

**Phase 1: Core Infrastructure** âœ… (Week 1-2)
- [x] Install Groq SDK
- [x] Create Groq service (`lib/ai/groq.ts`)
- [x] Create Gemini service (`lib/ai/gemini.ts`)
- [x] Implement confidence-based router (`lib/ai/confidence-router.ts`)
- [x] Add telemetry logging (`lib/telemetry/ai-logger.ts`)
- [x] Create chat API endpoint (`app/api/ai/chat/route.ts`)
- [x] Create telemetry API endpoint (`app/api/telemetry/ai/route.ts`)

**Phase 2: SEC Filing Ingestion** (Week 2-3)
- [ ] Implement EDGAR fetcher (`lib/api/secEdgar.ts` - basic exists, needs enhancement)
- [ ] Build PDF/HTML preprocessor (`lib/preprocessing/filings.ts`)
- [ ] Create chunking utility (`lib/preprocessing/chunker.ts`)
- [ ] Integrate Groq for summarization
- [ ] Set up Redis caching (`lib/cache/redis.ts`)

**Phase 3: Embeddings & RAG** (Week 3-4)
- [ ] Implement embedding service (`lib/ai/embeddings.ts`)
- [ ] Integrate FAISS (`lib/vector/faiss.ts`)
- [ ] Build lazy indexing pipeline
- [ ] Test semantic search with sample queries

**Phase 4: News Pipeline** (Week 4-5)
- [ ] Aggregate news sources (Finnhub already integrated at `lib/dao/finnhub.dao.ts`)
- [ ] Build batch sentiment pipeline (`scripts/batch-news.ts`)
- [ ] Set up cron jobs (8am, 12pm, 6pm ET)
- [ ] Test news digests with cache

**Phase 5: UI Integration** (Week 5-6)
- [ ] Update StonksAI component to use new chat endpoint
- [ ] Build cost tracking dashboard
- [ ] Add confidence indicators in UI
- [ ] Display cited sources

**Phase 6: Monitoring** (Week 6+)
- [ ] Implement Slack/email alerts for metric thresholds
- [ ] Create admin dashboard for telemetry
- [ ] Add user feedback collection (thumbs up/down)
- [ ] Build feedback loop for model tuning

---

## Testing

**Test Groq integration:**

```bash
npx tsx scripts/test-groq.ts
```

**Test Gemini integration:**

```bash
npx tsx scripts/test-gemini.ts
```

**Test confidence router:**

```bash
curl -X POST http://localhost:3000/api/ai/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What are TSLA risk factors?",
    "ragContext": "[Chunk 1] TSLA 10-K Item 1A: Risk factors include regulatory compliance..."
  }'
```

**Check telemetry:**

```bash
curl http://localhost:3000/api/telemetry/ai?period=1h
```

---

## Architecture Diagrams

### Two-Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PIPELINE A: Batch Processing (NO RAG)                      â”‚
â”‚                                                              â”‚
â”‚  SEC EDGAR â†’ Preprocess â†’ Chunk â†’ Groq Summarize â†’ Cache   â”‚
â”‚  News APIs â†’ Preprocess â†’ Chunk â†’ Groq Sentiment â†’ Cache   â”‚
â”‚                                                              â”‚
â”‚  âŒ NO embeddings   âŒ NO vector DB   âœ… Fast & cheap       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PIPELINE B: Investor Chat (YES RAG)                        â”‚
â”‚                                                              â”‚
â”‚  User Question â†’ Lazy Index (S3 chunks â†’ Embed â†’ FAISS)    â”‚
â”‚               â†’ Retrieve top-k chunks                       â”‚
â”‚               â†’ Gemini Flash (confidence-based)             â”‚
â”‚               â†’ Escalate to Pro if needed                   â”‚
â”‚               â†’ Cache answer                                â”‚
â”‚                                                              â”‚
â”‚  âœ… Embeddings only for chat   âœ… Lazy indexing   âœ… Cited  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Confidence-Based Routing Flow

```
User Question
    â†“
Check Cache (12h TTL)
    â†“ (miss)
Retrieve RAG Context (if available)
    â†“
Call Gemini Flash (temp=0.3)
    â†“
Confidence â‰¥0.85?
    â”œâ”€ YES â†’ Accept & Cache âœ…
    â””â”€ NO â†’ Confidence â‰¥0.60?
            â”œâ”€ YES â†’ Escalate to Gemini Pro â¬†ï¸
            â”‚        (temp=0.0, retry)
            â”‚        â†“
            â”‚    Improved? â†’ Accept & Cache âœ…
            â”‚        â†“
            â”‚    Still low? â†’ Flag for review âš ï¸
            â””â”€ NO â†’ Escalate to Gemini Pro â¬†ï¸
                    Flag for review âš ï¸
```

---

## References

- **AI Strategy:** [docs/AI_MODEL_STRATEGY.md](../../docs/AI_MODEL_STRATEGY.md)
- **System Design:** [docs/retail_stock_ai_pipeline_system_design_recommendations.md](../../docs/retail_stock_ai_pipeline_system_design_recommendations.md)
- **Groq Docs:** https://console.groq.com/docs
- **Google Gemini Docs:** https://ai.google.dev/docs
- **FAISS:** https://github.com/facebookresearch/faiss

---

*Last updated: November 20, 2025*
*Implemented by: Claude (AI Assistant)*
