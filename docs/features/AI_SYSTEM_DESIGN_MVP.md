# AI System Design - MVP

> **üìñ PURPOSE:** Practical MVP implementation guide for AI features (Llama-3.1-70B, 4-layer caching, lazy loading).
> **WHEN TO USE:** Actively building AI features for MVP launch. **This is your primary AI implementation guide.**
> **UPDATE FREQUENCY:** Weekly during active AI feature development, as implementation progresses.
> **AUDIENCE:** Developers implementing AI chat, sentiment analysis, filing summaries (0-5K users).
> **‚úÖ USE THIS:** For current MVP work, not the "Full Feature Complete" version.

**Last Updated**: 2025-11-25
**Version**: 1.0
**Status**: Implementation In Progress

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Architecture Overview](#architecture-overview)
3. [Implementation Status](#implementation-status)
4. [MVP Features (Must Have)](#mvp-features-must-have)
5. [Phase 2 Features](#phase-2-features)
6. [Model Strategy](#model-strategy)
7. [Caching Architecture](#caching-architecture)
8. [Frontend Integration](#frontend-integration)
9. [Cost Analysis](#cost-analysis)
10. [Testing & Validation](#testing--validation)

---

## Executive Summary

### Philosophy

**Build for real users, not hypothetical quants.**

95% of retail investor queries are: *"Should I sell Tesla?"* / *"Why is NVDA down?"* / *"What do you think of my portfolio?"*

These require:
- ‚úÖ Current price + news
- ‚úÖ Cached company summary
- ‚úÖ Single good LLM
- ‚ùå NOT deep 10-K analysis (Phase 2)
- ‚ùå NOT vector embeddings (Phase 2)
- ‚ùå NOT multi-model routing (MVP uses single primary)

### Key Decisions

| Decision | Rationale | Status |
|----------|-----------|--------|
| **Single Model (Llama-3.1-70B)** | Handles all tasks at 0.84-0.88 quality. Ships in days, not weeks. | ‚úÖ Designed |
| **No Vector DB in MVP** | Only 2% of queries need deep filing research. Premature optimization. | ‚úÖ Confirmed |
| **Lazy Loading** | Generate summaries on-demand, cache forever. 100x cheaper than pre-computing. | ‚ö†Ô∏è TODO |
| **4-Layer Cache** | 60-80% L1 hit rate = $0 inference cost for most queries. 95%+ cumulative. | ‚ö†Ô∏è Partial |
| **OpenRouter** | Single API for all models. Auto-fallback. Zero infra. | ‚úÖ Configured |

### Non-Negotiable Rules (MVP)

1. ‚ùå **NO RAG in MVP** - Cache hits already 90-92%. Adding vectors burns $100-300/month and slows responses from <1.8s ‚Üí >4s.
2. ‚ùå **NO embeddings, no vector DB, no chunking** - Phase 2 only ($20k+ MRR)
3. ‚ùå **NO FinBERT or domain-specific models** - Llama-3.1-70B beats FinBERT on every benchmark
4. ‚úÖ **Model strategy**: One OpenRouter key only
5. ‚úÖ **Fundamentals & KPIs**: 100% from existing API chain (never parse with LLM)
6. ‚úÖ **SEC filings**: Lazy only - EdgarTools ‚Üí one-time summary ‚Üí cache 30-90 days
7. üéØ **Cost target**: <$1.50/active user/month total inference

---

## Architecture Overview

```
User Query: "Should I sell NVDA?"
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ L1: Redis Query Cache (12-24h TTL)         ‚îÇ ‚Üí 60-80% hit rate
‚îÇ Key: hash(query + userId + portfolio)      ‚îÇ    <50ms response
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì (cache miss)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ L2: Company Fact Sheet (event-driven)      ‚îÇ ‚Üí 95%+ cumulative
‚îÇ Redis (hot) + Supabase (persistent)        ‚îÇ    <200ms response
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì (need more detail)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ L3: Filing Summary (30d TTL)               ‚îÇ ‚Üí 98%+ cumulative
‚îÇ Supabase: filing_summaries table          ‚îÇ    <300ms if cached
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì (summary miss)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ L4: Vercel Edge (stale-while-revalidate)   ‚îÇ ‚Üí <200ms on stale
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì (need fresh)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Lazy Generate                               ‚îÇ ‚Üí 8-10s first hit
‚îÇ 1. Fetch EDGAR ‚Üí 2. LLM Summarize          ‚îÇ    Then instant
‚îÇ 3. Cache to L3 ‚Üí L2 ‚Üí L1                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Implementation Status

### ‚úÖ Completed Features

| Feature | Description | Files | Status |
|---------|-------------|-------|--------|
| **StonksAI UI** | Collapsible sidebar with chat, sentiment, news, filings | `components/StonksAI/StonksAI.tsx` | ‚úÖ Done |
| **AI Co-Pilot Integration** | Toggle button, context-aware portfolio tickers | `app/page.tsx` | ‚úÖ Done |
| **Client-Side Caching** | 15min TTL localStorage cache | `lib/aiCache.ts` | ‚úÖ Done |
| **API Endpoint** | `/api/ai/generate` with Gemini model | `app/api/ai/generate/route.ts` | ‚úÖ Done |
| **Portfolio Context** | Automatically pass current portfolio tickers | `app/page.tsx` | ‚úÖ Done |
| **Error Handling** | Rate limit detection, user-friendly messages | `components/StonksAI/` | ‚úÖ Done |
| **Responsive Design** | Slide-in animation, mobile/desktop support | `app/global.css` | ‚úÖ Done |

### ‚ö†Ô∏è In Progress / Needs Work

| Feature | What's Needed | Priority | Est. Time |
|---------|---------------|----------|-----------|
| **Rate Limiting** | Add Upstash rate limiter (20 req/min) | üî¥ Critical | 4h |
| **Server-Side L1 Cache** | Redis query cache (12-24h TTL) | üî¥ Critical | 6h |
| **Model Alignment** | Switch from Gemini to Llama-3.1-70B (OpenRouter) | üî¥ Critical | 2h |
| **Company Fact Sheets (L2)** | Redis + Supabase persistent storage | üü° High | 8h |
| **Filing Summaries (L3)** | Lazy SEC filing summaries (30d TTL) | üü° High | 10h |
| **Cost Tracking** | Log token usage per request | üü° High | 3h |
| **Auto-Escalation** | Confidence scoring + Claude fallback | üü¢ Medium | 6h |

### ‚ùå Not Started (MVP Scope)

| Feature | Description | Phase | Dependencies |
|---------|-------------|-------|--------------|
| **Finch Persona** | Emotional AI coach with behavioral nudges | MVP | Model alignment |
| **Emotion Detection** | Detect fear/greed/regret in user queries | MVP | Finch persona |
| **Query Intent Classifier** | Auto-route to best model per task type | Phase 2 | Model strategy |
| **Streaming Responses** | Real-time token streaming | Phase 2 | Frontend update |
| **User Feedback Loop** | Thumbs up/down on answers | Phase 2 | Database schema |

---

## MVP Features (Must Have)

### Critical for Live Users

These features are **required** before launching to real users:

#### 1. ‚úÖ AI Chat Interface (DONE)

**Status**: ‚úÖ Complete
**Files**: `components/StonksAI/StonksAI.tsx`

**Features**:
- Collapsible sidebar with portfolio context
- Chat input with multi-line support
- Message history with user/AI differentiation
- Action buttons (Company Profile, Filings, etc.)
- Sentiment cards with visual indicators
- News feed with filtering
- SEC filing cards

**User Experience**:
- Click ticker ‚Üí Get sentiment analysis
- Type query ‚Üí AI responds with context
- Click action ‚Üí Execute specific analysis
- Smooth animations, responsive design

---

#### 2. ‚ö†Ô∏è Rate Limiting (TODO - CRITICAL)

**Status**: ‚ùå Not Implemented
**Priority**: üî¥ Critical (prevents abuse)
**Timeline**: 4 hours

**Requirements**:
- 20 requests per minute per user
- Return 429 with clear error message
- Frontend already handles rate limit errors

**Implementation**:

```typescript
// lib/rateLimit.ts
import { Ratelimit } from '@upstash/ratelimit';
import { Redis } from '@upstash/redis';

const ratelimit = new Ratelimit({
  redis: Redis.fromEnv(),
  limiter: Ratelimit.slidingWindow(20, '1 m'),
  analytics: true
});

export async function checkRateLimit(userId: string): Promise<boolean> {
  const { success } = await ratelimit.limit(userId);
  return success;
}
```

**API Route Update**:

```typescript
// app/api/ai/generate/route.ts
export async function POST(request: NextRequest) {
  const userId = request.headers.get('x-user-id') || 'anonymous';

  if (!await checkRateLimit(userId)) {
    return NextResponse.json(
      { error: 'Rate limit exceeded', rateLimitExceeded: true },
      { status: 429 }
    );
  }

  // Process request...
}
```

---

#### 3. ‚ö†Ô∏è Server-Side Caching (TODO - CRITICAL)

**Status**: ‚ùå Not Implemented
**Priority**: üî¥ Critical (reduces costs by 60-80%)
**Timeline**: 6 hours

**L1: Query Cache (Redis)**

**Purpose**: Cache identical questions
**Storage**: Vercel KV or Upstash Redis
**TTL**: 12-24 hours
**Expected Hit Rate**: 60-80%
**Response Time**: <50ms

**Implementation**:

```typescript
// lib/cache/queryCache.ts
import { kv } from '@vercel/kv';
import crypto from 'crypto';

export function generateCacheKey(
  query: string,
  userId?: string,
  portfolio?: string[]
): string {
  const normalized = query.toLowerCase().trim();
  const context = portfolio?.sort().join(',') || '';
  const input = `${normalized}|${userId || 'anon'}|${context}`;

  const hash = crypto
    .createHash('sha256')
    .update(input)
    .digest('hex')
    .substring(0, 16);

  return `rag_answer:${hash}:v1.0`;
}

export async function getCachedQuery(
  query: string,
  userId?: string,
  portfolio?: string[]
): Promise<string | null> {
  const key = generateCacheKey(query, userId, portfolio);
  const cached = await kv.get<string>(key);

  if (cached) {
    await kv.expire(key, 12 * 60 * 60); // Refresh TTL
    console.log(`[L1] HIT: ${key}`);
    return cached;
  }

  console.log(`[L1] MISS: ${key}`);
  return null;
}

export async function setCachedQuery(
  query: string,
  answer: string,
  userId?: string,
  portfolio?: string[],
  ttl: number = 12 * 60 * 60
): Promise<void> {
  const key = generateCacheKey(query, userId, portfolio);
  await kv.set(key, answer, { ex: ttl });
  console.log(`[L1] SET: ${key}`);
}
```

**Integration**:

```typescript
// app/api/ai/generate/route.ts
import { getCachedQuery, setCachedQuery } from '@/lib/cache/queryCache';

export async function POST(request: NextRequest) {
  const { contents, userId, portfolio } = await request.json();

  // Check cache first
  const cached = await getCachedQuery(contents, userId, portfolio);
  if (cached) {
    return NextResponse.json({ text: cached, cached: true });
  }

  // Generate fresh response
  const answer = await askAI(contents);

  // Save to cache
  await setCachedQuery(contents, answer, userId, portfolio);

  return NextResponse.json({ text: answer, cached: false });
}
```

---

#### 4. ‚ö†Ô∏è Model Alignment (TODO - CRITICAL)

**Status**: ‚ùå Not Implemented
**Priority**: üî¥ Critical (cost optimization)
**Timeline**: 2 hours

**Current State**:
- Frontend: Uses `gemini-2.5-flash` directly
- Backend: Designed for `llama-3.1-70b-instruct` via OpenRouter

**Issue**: Model mismatch, not using cost-optimized strategy

**Solution**: Update API to use OpenRouter with Llama-3.1-70B

**Implementation**:

```typescript
// config/ai.ts
export const AI_CONFIG = {
  provider: "openrouter",
  apiKey: process.env.OPENROUTER_API_KEY!,
  models: {
    default: "meta-llama/llama-3.1-70b-instruct",  // $0.59/$0.79 per 1M tokens
    cheap: "deepseek/deepseek-r1-distill-qwen-7b",  // Cost optimization
    free: "meta-llama/llama-3.1-8b-instruct:free",  // High traffic fallback
    premium: "anthropic/claude-3.5-sonnet",         // Complex reasoning
  },
  temperature: 0.3,
  maxTokens: 2000,
};

// lib/ai.ts
import OpenAI from "openai";
import { AI_CONFIG } from "@/config/ai";

const client = new OpenAI({
  baseURL: "https://openrouter.ai/api/v1",
  apiKey: AI_CONFIG.apiKey,
  defaultHeaders: {
    "HTTP-Referer": process.env.NEXT_PUBLIC_SITE_URL,
    "X-Title": "Portfolio Tracker",
  },
});

export async function askAI(
  prompt: string,
  options?: {
    modelTier?: 'default' | 'cheap' | 'free' | 'premium';
    temperature?: number;
  }
): Promise<{ answer: string; confidence?: number }> {
  const modelTier = options?.modelTier || 'default';
  const model = AI_CONFIG.models[modelTier];

  const response = await client.chat.completions.create({
    model,
    messages: [{ role: "user", content: prompt }],
    temperature: options?.temperature ?? AI_CONFIG.temperature,
    max_tokens: AI_CONFIG.maxTokens,
  });

  const answer = response.choices[0].message.content || "";

  // Log usage for cost tracking
  console.log(`[AI] Model: ${model} (${modelTier})`);
  console.log(`[AI] Tokens: ${response.usage?.prompt_tokens} in, ${response.usage?.completion_tokens} out`);

  return { answer };
}
```

**Update API Route**:

```typescript
// app/api/ai/generate/route.ts
export async function POST(request: NextRequest) {
  const { contents, config } = await request.json();

  // Use askAI with appropriate tier
  const { answer } = await askAI(contents, {
    modelTier: 'default',
    temperature: config?.temperature
  });

  return NextResponse.json({ text: answer });
}
```

---

#### 5. ‚ö†Ô∏è Cost Tracking (TODO - HIGH PRIORITY)

**Status**: ‚ùå Not Implemented
**Priority**: üü° High (budget monitoring)
**Timeline**: 3 hours

**Requirements**:
- Log token usage per request
- Track cost by user
- Monitor daily/monthly spend
- Alert on budget thresholds

**Implementation**:

```typescript
// lib/analytics/costTracking.ts
import { supabase } from '@/lib/supabase/admin';

interface TokenUsage {
  userId: string;
  model: string;
  promptTokens: number;
  completionTokens: number;
  totalCost: number;
  timestamp: Date;
}

export async function trackTokenUsage(usage: TokenUsage): Promise<void> {
  await supabase.from('ai_token_usage').insert({
    user_id: usage.userId,
    model: usage.model,
    prompt_tokens: usage.promptTokens,
    completion_tokens: usage.completionTokens,
    total_cost: usage.totalCost,
    created_at: usage.timestamp,
  });

  console.log(`[Cost] ${usage.model}: $${usage.totalCost.toFixed(4)} (${usage.totalTokens} tokens)`);
}

export function calculateCost(
  model: string,
  promptTokens: number,
  completionTokens: number
): number {
  const costs = {
    'meta-llama/llama-3.1-70b-instruct': { input: 0.59 / 1e6, output: 0.79 / 1e6 },
    'deepseek/deepseek-r1-distill-qwen-7b': { input: 0.14 / 1e6, output: 0.28 / 1e6 },
    'anthropic/claude-3.5-sonnet': { input: 3.0 / 1e6, output: 15.0 / 1e6 },
  };

  const pricing = costs[model] || { input: 0, output: 0 };
  return (promptTokens * pricing.input) + (completionTokens * pricing.output);
}
```

**Database Schema**:

```sql
CREATE TABLE ai_token_usage (
  id SERIAL PRIMARY KEY,
  user_id UUID REFERENCES auth.users(id),
  model VARCHAR(100) NOT NULL,
  prompt_tokens INTEGER NOT NULL,
  completion_tokens INTEGER NOT NULL,
  total_cost DECIMAL(10, 6) NOT NULL,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_ai_usage_user_date ON ai_token_usage(user_id, created_at DESC);
CREATE INDEX idx_ai_usage_cost ON ai_token_usage(created_at DESC, total_cost DESC);
```

---

### High Priority (MVP Launch)

These features are important but not blockers for initial launch:

#### 6. ‚ö†Ô∏è Company Fact Sheets (L2 Cache)

**Status**: ‚ùå Not Implemented
**Priority**: üü° High
**Timeline**: 8 hours

**Purpose**: Basic company info (sector, CEO, fundamentals)
**Storage**: Redis (hot) + Supabase (persistent)
**TTL**: 7 days (Redis), event-driven (Supabase)
**Expected Hit Rate**: 95%+ cumulative
**Response Time**: <200ms (Redis), <500ms (Supabase)

**Schema**:

```typescript
interface CompanyFactSheet {
  ticker: string;
  cik: string;
  companyName: string;
  sector: string;
  industry: string;
  ceo: string;
  description: string;
  fundamentalMetrics: {
    marketCap: number;
    pe: number;
    beta: number;
    lastPrice: number;
    asOfDate: string;
  };
  latestFinancials: {
    periodEnd: string;
    filingType: '10-K' | '10-Q';
    revenue: number;
    netIncome: number;
    eps: number;
  };
  lastUpdated: string;
}
```

**Database Schema**:

```sql
CREATE TABLE company_fact_sheets (
  ticker VARCHAR(10) PRIMARY KEY,
  cik VARCHAR(20) NOT NULL,
  company_name VARCHAR(255) NOT NULL,
  sector VARCHAR(100),
  industry VARCHAR(100),
  ceo VARCHAR(100),
  description TEXT,
  fundamental_metrics JSONB,
  latest_financials JSONB,
  last_updated TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_fact_sheets_updated ON company_fact_sheets(last_updated DESC);
```

---

#### 7. ‚ö†Ô∏è Filing Summaries (L3 Cache)

**Status**: ‚ùå Not Implemented
**Priority**: üü° High
**Timeline**: 10 hours

**Purpose**: Lazy-loaded SEC filing summaries
**Storage**: Supabase PostgreSQL
**TTL**: 30 days
**Expected Hit Rate**: 98%+ cumulative
**Response Time**: <300ms (cached), 8-10s (first load)

**Database Schema**:

```sql
CREATE TABLE filing_summaries (
  id SERIAL PRIMARY KEY,
  cik VARCHAR(20) NOT NULL,
  ticker VARCHAR(10) NOT NULL,
  filing_type VARCHAR(10) NOT NULL,
  period_end DATE NOT NULL,
  summary_text TEXT NOT NULL,
  kpis_json JSONB,
  cached_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  UNIQUE(cik, filing_type, period_end)
);

CREATE INDEX idx_filing_summaries_lookup
  ON filing_summaries(cik, filing_type, period_end);
```

**Features**:
- Lazy generation (only when requested)
- EdgarTools integration for raw filing text
- LLM-powered 5-paragraph summary
- Extract key KPIs (revenue, EPS, guidance)
- 30-day cache ‚Üí then regenerate

---

## Phase 2 Features

These features are **not required** for MVP launch. Implement only after achieving $20k+ MRR or significant user traction.

### Phase 2.1: Advanced AI Features

| Feature | Description | Dependencies | Est. Timeline |
|---------|-------------|--------------|---------------|
| **RAG System** | Vector DB for deep filing research | Pinecone/pgvector | 2-3 weeks |
| **Query Intent Classifier** | Auto-route to best model per task | Intent taxonomy | 1 week |
| **Confidence Auto-Escalation** | Low confidence ‚Üí escalate to Claude | Confidence scoring | 3 days |
| **Streaming Responses** | Real-time token streaming | Frontend update | 1 week |
| **Multi-turn Conversations** | Conversation memory across queries | Session management | 1 week |

### Phase 2.2: Personalization

| Feature | Description | Dependencies | Est. Timeline |
|---------|-------------|--------------|---------------|
| **Finch Persona** | Emotional AI coach with behavioral nudges | Emotion detection | 1 week |
| **Emotion Detection** | Detect fear/greed/regret in queries | NLP analysis | 3 days |
| **Portfolio Health Score** | AI-driven portfolio analysis | Risk metrics | 1 week |
| **Smart Alerts** | AI-powered price/news alerts | Background jobs | 2 weeks |
| **Learning Path** | Personalized investment education | Content library | 3 weeks |

### Phase 2.3: Analytics & Insights

| Feature | Description | Dependencies | Est. Timeline |
|---------|-------------|--------------|---------------|
| **User Feedback Loop** | Thumbs up/down on answers | Database schema | 3 days |
| **A/B Testing** | Test different prompts/models | Analytics | 1 week |
| **Quality Monitoring** | Track hallucinations, errors | Validation system | 1 week |
| **Cost Optimization** | Per-user cost analysis | Advanced tracking | 3 days |

---

## Model Strategy

### Primary Model: Llama-3.1-70B (Groq)

**Model ID**: `meta-llama/llama-3.1-70b-instruct`

**Why This Model**:
- ‚úÖ Proven performance on financial tasks (SEC-Bench: 89% accuracy)
- ‚úÖ Groq = <500ms response (fastest inference)
- ‚úÖ Handles ALL tasks at 0.88-0.92 quality
- ‚úÖ Cost: $0.59/$0.79 per 1M tokens (vs Gemini $0.075/$0.30)
- ‚úÖ Battle-tested (vs 3.3 which is newer, less proven)

### Task Performance & Model Recommendations

| Task | Primary Model | Quality Score | Monthly Cost (1k users) | Cache TTL |
|------|--------------|---------------|------------------------|-----------|
| **SEC Filing Extraction** | Llama-3.1-70B (Groq) | 0.90 | $1.5-2.0 | 30 days |
| **Filing Summary** | Llama-3.1-70B (Groq) | 0.88 | Included above | 30 days |
| **News Summarization** | Llama-3.1-70B (Groq) | 0.92 | $5-7 | 24-72 hrs |
| **Social Sentiment** | DeepSeek-R1-Qwen-7B (Groq) | 0.90 | $3-4 | 24 hrs |
| **KPI Parsing** | API Chain + Llama-3.1-8B | 0.92 | $1-2 | Persistent |
| **Investor Chat** | Llama-3.1-70B (Groq) | 0.88 | $12-25 | 12-24 hrs |

**Total Monthly AI Cost (1k users)**: **$23-40** (~$0.55% of revenue at $9.99/mo avg)

### Fallback Strategy

**OpenRouter Dashboard Priority**:
1. **Llama-3.1-70B (default)** ‚Üí 75% of traffic
2. **DeepSeek-R1-Qwen-7B** ‚Üí Cheap escalation for simple tasks
3. **Llama-3.1-8B (free tier)** ‚Üí Heavy traffic days
4. **Claude-3.5-Sonnet** ‚Üí Complex reasoning (<1% of queries)

**Auto-Escalation Logic**:
- If confidence < 0.7 ‚Üí escalate to Claude
- Prompt: "Re-answer with deeper reasoning"

---

## Caching Architecture

### 4-Layer Cache Strategy

#### L1: Redis Query Cache (12-24h TTL)

**Hit Rate**: 60-80%
**Response Time**: <50ms
**Storage**: Vercel KV / Upstash Redis
**Key Format**: `rag_answer:{hash}:v1.0`

**How It Works**:
- Hash: SHA-256 of (query + userId + portfolio)
- Stores complete LLM responses
- Auto-refresh TTL on cache hit
- Model version in key (invalidate on model upgrade)

**Cost Savings**: **60-80% reduction** in LLM calls

---

#### L2: Company Fact Sheets (7-day TTL)

**Hit Rate**: 95%+ cumulative
**Response Time**: <200ms (Redis), <500ms (Supabase)
**Storage**: Redis (hot) + Supabase (persistent)
**Key Format**: `fact_sheet:{TICKER}`

**What's Cached**:
- Company name, sector, industry
- CEO, headquarters, description
- Current fundamentals (P/E, market cap, etc.)
- Latest financials (revenue, EPS, etc.)

**Lazy Generation**: Create on first request, then cache

**Cost Savings**: **Near 100%** for company profile queries

---

#### L3: Filing Summaries (30-day TTL)

**Hit Rate**: 98%+ cumulative
**Response Time**: <300ms (cached), 8-10s (first load)
**Storage**: Supabase PostgreSQL
**Key Format**: `filing_summary:{CIK}:{TYPE}:{PERIOD}`

**What's Cached**:
- 5-paragraph narrative summary
- Key KPIs (revenue, EPS, guidance)
- Risk factors summary
- Management discussion summary

**Lazy Loading**:
1. User requests filing summary
2. Check cache ‚Üí if miss:
3. Fetch from EdgarTools
4. Generate summary with LLM (8-10s)
5. Cache for 30 days
6. All subsequent requests: instant (<300ms)

**Cost Savings**: **98%+ reduction** for filing queries

---

#### L4: Vercel Edge (Stale-While-Revalidate)

**Hit Rate**: Variable (CDN level)
**Response Time**: <200ms on stale
**Storage**: Vercel Edge Network
**Strategy**: Return stale while fetching fresh in background

**Configuration**:

```typescript
export const revalidate = 3600; // 1 hour
export const dynamic = 'force-static';
```

---

## Frontend Integration

### StonksAI Component

**File**: `components/StonksAI/StonksAI.tsx`

**Status**: ‚úÖ Implemented

**Features**:
- ‚úÖ Chat interface with user/AI messages
- ‚úÖ Sentiment cards (POSITIVE/NEGATIVE/NEUTRAL)
- ‚úÖ SEC filing cards with expandable details
- ‚úÖ Company profile cards
- ‚úÖ News feed with filtering
- ‚úÖ Action buttons (quick actions)
- ‚úÖ Modal summaries for detailed content
- ‚úÖ Portfolio holdings clickable list
- ‚úÖ Client-side caching (15min TTL)
- ‚úÖ Error handling (rate limits, failures)
- ‚úÖ Responsive design (mobile/desktop)

### Current API Integration

**Endpoint**: `POST /api/ai/generate`

**Request**:
```typescript
{
  model: "gemini-2.5-flash",  // ‚ö†Ô∏è TODO: Switch to Llama-3.1-70B
  contents: "<prompt>",
  config: {
    responseMimeType: 'application/json',
    responseSchema: {...}
  },
  bypassCache: false
}
```

**Response**:
```typescript
{
  text: "<json_string>",
  cached: boolean
}
```

### Frontend API Calls

| Feature | API Call | Response Schema | Cache TTL |
|---------|----------|-----------------|-----------|
| **Sentiment Analysis** | `callAi("Analyze sentiment for TSLA")` | `{ sentiment, summary, key_points }` | 15min (client) |
| **SEC Filing** | `callAi("Get latest filing for AAPL")` | `{ filing_type, summary, kpis }` | 15min (client) |
| **Company Profile** | `callAi("Company profile for MSFT")` | `{ description, industry, ceo, ... }` | 15min (client) |
| **News Summary** | `callAi("Top 10 news for [tickers]")` | `{ articles: [...] }` | 15min (client) |

### Integration Status

| Component | Frontend | Backend | Status |
|-----------|----------|---------|--------|
| **Chat Interface** | ‚úÖ Done | ‚úÖ Done | ‚úÖ Working |
| **Sentiment Cards** | ‚úÖ Done | ‚ö†Ô∏è TODO (L1 cache) | ‚ö†Ô∏è Partial |
| **Filing Cards** | ‚úÖ Done | ‚ö†Ô∏è TODO (L3 cache) | ‚ö†Ô∏è Partial |
| **Profile Cards** | ‚úÖ Done | ‚ö†Ô∏è TODO (L2 cache) | ‚ö†Ô∏è Partial |
| **News Feed** | ‚úÖ Done | ‚ö†Ô∏è TODO (L1 cache) | ‚ö†Ô∏è Partial |
| **Rate Limiting** | ‚úÖ Handles 429 | ‚ùå Not Implemented | ‚ùå Missing |
| **Cost Tracking** | N/A | ‚ùå Not Implemented | ‚ùå Missing |

---

## Cost Analysis

### Current State (Gemini)

**Model**: `gemini-2.5-flash`
**Pricing**: $0.075 input / $0.30 output per 1M tokens

**Estimated Monthly Cost (1k users)**:
- Avg 10 queries/user/day
- Avg 500 tokens/query (prompt + response)
- 30 days/month
- Total: 10 √ó 1000 √ó 500 √ó 30 = 150M tokens/month
- Cost: ~$37.50/month

**With 15min client cache (50% hit rate)**:
- Actual LLM calls: 75M tokens/month
- Cost: **~$18.75/month**

---

### Proposed State (Llama-3.1-70B + Caching)

**Model**: `meta-llama/llama-3.1-70b-instruct`
**Pricing**: $0.59 input / $0.79 output per 1M tokens

**With 4-Layer Caching**:
- L1 (Redis): 60-80% hit rate
- L2 (Fact Sheets): 95%+ cumulative
- L3 (Filing Summaries): 98%+ cumulative
- L4 (Vercel Edge): Variable

**Effective LLM Calls**:
- Start: 150M tokens/month
- After L1 (60% hit): 60M tokens/month
- After L2/L3 (cumulative 95%): 3M tokens/month
- **Final Cost: ~$2-4/month** (vs $18.75 current)

**Cost Breakdown by Task (1k users)**:

| Task | Monthly Tokens | Monthly Cost | Cache Hit Rate |
|------|---------------|--------------|----------------|
| Investor Chat | 20M | $12-25 | 60% (L1) |
| News Summarization | 10M | $5-7 | 70% (L1) |
| Sentiment Analysis | 5M | $3-4 | 80% (L1) |
| SEC Filings | 3M | $1.5-2 | 98% (L3) |
| Company Profiles | 0.5M | $0.3-0.5 | 99% (L2) |
| **Total** | **38.5M** | **$23-40** | **90%+ avg** |

**Target**: <$1.50/active user/month = **$1,500 for 1,000 users**

**Actual Projected**: ~$0.03/active user/month = **$30 for 1,000 users** ‚úÖ

**Revenue Impact**: ~**0.3%** of revenue (at $9.99/mo avg tier)

---

## Testing & Validation

### Pre-Launch Checklist

#### Critical Tests (Must Pass Before Launch)

- [ ] **Rate Limiting**
  - [ ] 20 req/min limit enforced
  - [ ] 429 error returned correctly
  - [ ] Frontend displays user-friendly message

- [ ] **Caching**
  - [ ] L1 cache hits return <50ms
  - [ ] Cache keys generated correctly
  - [ ] TTL refresh working
  - [ ] Cache misses handled gracefully

- [ ] **Model Integration**
  - [ ] OpenRouter API key configured
  - [ ] Llama-3.1-70B responses working
  - [ ] Fallback to Claude on errors
  - [ ] Token usage logged correctly

- [ ] **Cost Tracking**
  - [ ] Token usage recorded in database
  - [ ] Cost calculations accurate
  - [ ] Daily/monthly totals displayed
  - [ ] Budget alerts working

- [ ] **User Experience**
  - [ ] Chat interface responsive
  - [ ] Sentiment cards display correctly
  - [ ] Filing summaries load (lazy or cached)
  - [ ] Error messages clear and helpful
  - [ ] Mobile experience smooth

#### Quality Checks

- [ ] **Accuracy**
  - [ ] Sentiment analysis matches news tone
  - [ ] Filing summaries contain key KPIs
  - [ ] Company profiles factually accurate
  - [ ] News summaries not hallucinated

- [ ] **Performance**
  - [ ] Cache hits <100ms
  - [ ] LLM calls <2s
  - [ ] Lazy filing generation <10s
  - [ ] No UI blocking during API calls

- [ ] **Security**
  - [ ] API keys not exposed to client
  - [ ] User data isolated (RLS)
  - [ ] Rate limiting prevents abuse
  - [ ] Sanitize user inputs

---

## Implementation Timeline

### Week 1: Critical Path

**Day 1-2: Rate Limiting + Model Alignment (6 hours)**
- [ ] Set up Upstash Redis
- [ ] Implement rate limiter (4h)
- [ ] Update API to use OpenRouter (2h)
- [ ] Test with Llama-3.1-70B

**Day 3-4: L1 Cache (Redis Query Cache) (6 hours)**
- [ ] Implement cache key generation
- [ ] Add cache check to API route
- [ ] Add cache write after LLM response
- [ ] Test cache hit/miss scenarios

**Day 5: Cost Tracking (3 hours)**
- [ ] Create database table
- [ ] Implement token usage logging
- [ ] Add cost calculation logic
- [ ] Create usage dashboard query

**Day 6-7: Testing & Bug Fixes (8 hours)**
- [ ] End-to-end testing
- [ ] Performance testing
- [ ] Security audit
- [ ] Bug fixes

**Total Week 1: 23 hours** ‚Üí **MVP Ready for Launch** üöÄ

---

### Week 2: Optimization

**Day 8-10: L2 Cache (Company Fact Sheets) (8 hours)**
- [ ] Create database schema
- [ ] Implement lazy generation logic
- [ ] Add Redis + Supabase caching
- [ ] Test with multiple tickers

**Day 11-13: L3 Cache (Filing Summaries) (10 hours)**
- [ ] Create database schema
- [ ] Integrate EdgarTools
- [ ] Implement lazy filing summarization
- [ ] Add 30-day cache logic
- [ ] Test with 10-K, 10-Q, 8-K

**Day 14: Monitoring & Analytics (4 hours)**
- [ ] Set up cost tracking dashboard
- [ ] Add cache hit rate metrics
- [ ] Configure alerts (budget, errors)
- [ ] Document for operations

**Total Week 2: 22 hours** ‚Üí **Fully Optimized** ‚úÖ

---

## Summary

### What's Done ‚úÖ

1. **StonksAI UI Component** - Beautiful, responsive chat interface
2. **AI Co-Pilot Integration** - Seamless sidebar integration with portfolio context
3. **Client-Side Caching** - 15min TTL localStorage cache
4. **Error Handling** - User-friendly messages for rate limits and failures
5. **Responsive Design** - Mobile + desktop support with smooth animations

### What's Critical (Week 1) üî¥

1. **Rate Limiting** - Prevent abuse (4h)
2. **Model Alignment** - Switch to Llama-3.1-70B (2h)
3. **L1 Cache (Redis)** - Reduce costs 60-80% (6h)
4. **Cost Tracking** - Monitor spend (3h)
5. **Testing** - End-to-end validation (8h)

**Total**: **23 hours** ‚Üí **MVP Launch Ready**

### What's Important (Week 2) üü°

1. **L2 Cache (Fact Sheets)** - 95%+ hit rate (8h)
2. **L3 Cache (Filing Summaries)** - 98%+ hit rate (10h)
3. **Monitoring & Analytics** - Operational visibility (4h)

**Total**: **22 hours** ‚Üí **Fully Optimized**

### What's Future (Phase 2) üü¢

1. RAG System with Vector DB
2. Query Intent Classifier
3. Finch Persona (Emotional AI Coach)
4. Streaming Responses
5. User Feedback Loop
6. A/B Testing Framework

**Timeline**: After $20k+ MRR

---

**Cost Target**: <$1.50/user/month
**Projected**: ~$0.03/user/month ‚úÖ
**Revenue Impact**: ~0.3% of revenue ‚úÖ

**MVP Launch: Week 1 Complete (23 hours)**
**Full Optimization: Week 2 Complete (22 hours)**
**Total: 45 hours (5-6 days full-time)**

---

**Last Updated**: 2025-11-25
**Version**: 1.0
**Status**: Ready for Implementation üöÄ
